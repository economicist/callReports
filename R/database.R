#' Create a `function` that opens an SQLite connection on command.
#' 
#' `db_connector_sqlite()` returns a `function` that allows an SQLite connection
#' to be opened whenever necessary by other functions in this package.
#' 
#' @param sqlite_db_path The path to an SQLite database file. Must be in an
#' existing directory. The file will be created if it does not exist.
#' @return A `function` that opens a connection to the database given in
#' `sqlite_db_path`. It should be passed to other functions without the `()`
#' @export
#' @examples
#' db_connector_sqlite('./db/ffiec.sqlite')
db_connector_sqlite <- function(sqlite_db_path, overwrite = FALSE) {
  dir_name  <- dirname(sqlite_db_path)
  dir_name  <- ifelse(dir_name == '.', getwd(), dir_name)
  file_name <- basename(sqlite_db_path)
  
  if (!dir.exists(dir_name)) {
    stop(glue::glue('Directory `{dir_name}` does not exist. Confirm or create it.'))
  }
  
  if (file.exists(sqlite_db_path)) {
    journal_path <- glue::glue('{sqlite_db_path}-journal')
    if (file.exists(journal_path)) {
      cat(glue::glue('Database lock file {journal_path} detected.'), '\n')
      cat('This can be because there is an active transaction being performed\n')
      cat('on the database, or perhaps because a transaction was interrupted.\n')
      confirm_and_delete(journal_path)
    }
    if (overwrite) confirm_and_delete(sqlite_db_path)
  } else {
    cat(glue::glue('`{file_name}` does not exist in directory `{dir_name}`'), '\n')
    cat('Attempting to create it...\n')
    tryCatch(create_new_sqlite_db(sqlite_db_path), error = stop)
  }
  
  function() DBI::dbConnect(RSQLite::SQLite(), sqlite_db_path)
}

#' Create a new SQLite database to extract data into
#' 
#' If the SQLite database filename specified in `sqlite_db_path` already exists
#' and `overwrite` is set to `FALSE`, the user will be prompted to explicitly
#' permit overwriting it it.
#' 
#' `db_connector_sqlite(sqlite_db_path)` requires the path to a valid existing
#' SQLite database file. If one does not exist, one should be created using this
#' function before proceeding to extract data.
#' 
#' @param sqlite_db_path A path containing at least a valid SQLite filename. It
#' will be created in the current working directory if is not specified as part
#' of the filename.
#' @return A `function` that opens a connection to the newly created SQLite
#' database on demand
#' @export
#' @examples
#' # Create a new database file:
#' create_new_sqlite_db('./zips-ffiec/ffiec.sqlite')
create_new_sqlite_db <- function(sqlite_db_path) {
  # If file exists, ask for permission to overwrite. Exit if permission denied.
  confirm_and_delete(sqlite_db_path)
  
  # Try to open a connection to a new SQLite database at `sqlite_db_path`.
  # Stop execution if there's an error. Announce success otherwise.
  tryCatch({
    db_conn <- DBI::dbConnect(RSQLite::SQLite(), sqlite_db_path)
    DBI::dbDisconnect(db_conn)
  },
  error = stop)
  rlog::log_info(glue::glue('New SQLite database `{sqlite_db_path}` created.'))
  cat('\n')
}

#' Write extracted FFIEC schedule data to a database
#'
#' `write_ffiec_schedule()` writes the observation, codebook, and summary data
#' generated by `process_ffiec_schedule()` to the database whose connector is
#' given by `db_connector`.
#'
#' Uses a database "transaction" to write all three tables worth of data in one
#' go and only save the data if all of it succeeds. Either everything is written
#' to the database successfully, or nothing is. This prevents duplicate data
#' being entered into the database in case one reattempts to enter a schedule
#' after a hypothetical failed attempt that gets interrupted mid-write. Using a
#' transaction allows us to respond to any failure with an error by "rolling
#' back" (undoing) any new writes to the database and returning. If successful,
#' however, we "commit" (finalize) the changes to the database so that they'll
#' be visible in future queries.
#'
#' @param db_connector A `function` created by one of the `db_connector_*()`
#' functions found in this package. It should be passed without the `()`
#' @param tbl_name The name of the table you're writing, which should be a
#' valid schedule code.
#' @param df_obs A `tibble` containing the observations found in the extracted
#' schedule file, pivoted to long form.
#' `VALUE`)
#' @param df_codes A `tibble` containing the codebook information associated
#' with the extracted schedule file.
#' @param df_summ A `tibble` containing information about how many `IDRSSD` 
#' values are associated with each `VAR_CODE` pair in the schedule
#' @export
write_ffiec_schedule <- 
  function(db_connector, sch_code, df_obs, df_codes, df_summ) {
    tbl_name <- glue::glue('FFIEC.OBS_{sch_code}')
    db_conn <- db_connector()
    DBI::dbBegin(db_conn)
    tryCatch({
      DBI::dbWriteTable(db_conn, 'FFIEC.CODEBOOK', df_codes, append = TRUE)
      rlog::log_info(glue::glue('Writing {nrow(df_obs)} observations to the database...'))
      if (!DBI::dbExistsTable(db_conn, tbl_name)) {
        DBI::dbCreateTable(db_conn, tbl_name,
                           fields = c(IDRSSD      = 'INTEGER',
                                      QUARTER_ID  = 'INTEGER',
                                      VAR_CODE_ID = 'INTEGER',
                                      VALUE       = 'TEXT'))
      }
      DBI::dbWriteTable(db_conn, tbl_name, df_obs, append = TRUE)
      DBI::dbWriteTable(db_conn, 'FFIEC.SUMMARY', df_summ, append = TRUE)
      DBI::dbCommit(db_conn)
    },
    warning = function(w) {
      warning(w)
    },
    error = function(e) {
      DBI::dbRollback(db_conn)
      rlog::log_info('Error writing data to the database. No observations added:')
      stop(e)
    },
    finally = {
      DBI::dbDisconnect(db_conn)
      cat('\n')
    })
  }

#' Add variable names to an index table
#' 
#' `write_var_codes()` takes a list of variable codes and adds each of them
#' to a database table containing only a pairing between an integer ID and the
#' variable code. If the variable code is already in the table, it is skipped.
#' If it is not, it is automatically assigned an integer ID value in the order 
#' in which it is added.
#' 
#' Given the numerous variables in the source data, this library pivots the data
#' into long form before writing to the database. This offers the benefit of
#' allowing each table in the database to have a predictable description and 
#' prevent issues with exceeding the native maximum column support of many
#' database engines.
#' 
#' However, pivoting the data creates a text column with a copy of the variable
#' code being stored for every one of the millions of observations in the data
#' set, significantly expanding the storage requirement of the database, and
#' potentially making queries run noticeably slower on standard spinning-platter
#' hard drives due to the increased scanning time.
#' 
#' This function aims to alleviate that problem by assigning each variable code
#' an integer ID value so that the storage requirements are significantly
#' smaller.
#'
#' @param db_connector A `function` created by one of the `db_connector_*()`
#' functions found in this package. It should be passed without the `()`
#' @param var_codes 
#' @export
write_var_codes <- function(db_connector, var_codes) {
  `%not_in%` <- Negate(`%in%`)
  db_conn <- db_connector()
  
  # If the `VAR_CODES` table does not yet exist, create it with an ID column
  # that automatically increments with each new addition to the table, thus
  # automatically assigning each variable code an integer ID. Additionally,
  # prevent duplicate variable code entries by adding a `UNIQUE` constraint
  # to its relevant table column.
  if (!DBI::dbExistsTable(db_conn, 'VAR_CODES')) {
    tbl_gen_query <- 
      'CREATE TABLE VAR_CODES' %>%
      paste('(ID INTEGER PRIMARY KEY AUTOINCREMENT, VAR_CODE TEXT UNIQUE)')
   DBI::dbExecute(db_conn, tbl_gen_query)
  }
  
  # Fetch the variable codes already in the database (just the codes, not the IDs)
  existing_codes <- fetch_var_codes(db_connector) %>% getElement('VAR_CODE')
  
  # `ID` values are automatically assigned by the database engine to variable
  # codes, so here we assign `NA` values to them before sending the table to 
  # the database for writing.
  new_var_codes <- 
    tibble::tibble(ID       = rep(NA, length(var_codes)),
                   VAR_CODE = var_codes) %>%
    dplyr::filter(VAR_CODE %not_in% existing_codes)
  
  DBI::dbWriteTable(db_conn, 'VAR_CODES', new_var_codes, append = TRUE)
  DBI::dbDisconnect(db_conn)
}

#' Retrieve variable names from an index table
#'
#' `fetch_var_codes()` retrieves a character vector containing all the variable
#' codes that have thus far been assigned database ID values with `write_var_codes()`
#' 
#' Given the numerous variables in the source data, this library pivots the data
#' into long form before writing to the database. This offers the benefit of
#' allowing each table in the database to have a predictable description and 
#' prevent issues with exceeding the native maximum column support of many
#' database engines.
#' 
#' However, pivoting the data creates a text column with a copy of the variable
#' code being stored for every one of the millions of observations in the data
#' set, significantly expanding the storage requirement of the database, and
#' potentially making queries run noticeably slower on standard spinning-platter
#' hard drives due to the increased scanning time.
#' 
#' This function is part of an effort to alleviate that problem by assigning
#' each variable code an integer ID value so that the storage requirements are
#' significantly smaller.
#'
#' @param db_connector A `function` created by one of the `db_connector_*()`
#' functions found in this package. It should be passed without the `()`
#' @return A character vector of variable codes
#' @export
fetch_var_codes <- function(db_connector) {
  db_conn <- db_connector()
  if (!DBI::dbExistsTable(db_conn, 'VAR_CODES')) return(as.character(NULL))
  var_codes <-
    DBI::dbReadTable(db_conn, 'VAR_CODES') %>%
    dplyr::collect()
  DBI::dbDisconnect(db_conn)
  return(var_codes)
}
